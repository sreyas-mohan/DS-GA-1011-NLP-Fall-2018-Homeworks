{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './hw2_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "random.seed(134)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 150000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_label_map = {'neutral':0,\n",
    "#                 'contradiction': 1, \n",
    "#                 'entailment': 2};\n",
    "\n",
    "y_label_map = {'contradiction':0,'neutral':1,'entailment':2}\n",
    "\n",
    "def get_string_tokenized_data(data_mat_path):\n",
    "    df = pd.read_csv(os.path.join(data_mat_path), sep=\"\\t\")\n",
    "    data = np.array(df);\n",
    "    data = data.astype(str)\n",
    "    \n",
    "    tokenized_data_x = len(data) * [None];\n",
    "    y_labels = [y_label_map[x] for x in data[:, 2] ];\n",
    "    \n",
    "    all_tokens = [];\n",
    "    \n",
    "    for i,x in enumerate(data):\n",
    "        tokenized_data_x[i] = [x[0].split(), x[1].split()];\n",
    "        all_tokens += (tokenized_data_x[i][0] + tokenized_data_x[i][1])\n",
    "\n",
    "    return all_tokens, tokenized_data_x, y_labels\n",
    "        \n",
    "\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens1, tokens2 in tokens_data:\n",
    "        index_list1 = [token2id[token] if token in token2id else UNK_IDX for token in tokens1]\n",
    "        index_list2 = [token2id[token] if token in token2id else UNK_IDX for token in tokens2]\n",
    "        indices_data.append([index_list1, index_list2])\n",
    "    return indices_data\n",
    "\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    embedding_dict = np.random.randn(MAX_VOCAB_SIZE+2, EMBED_SIZE)\n",
    "    all_train_tokens = []\n",
    "    i = 0\n",
    "    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        all_train_tokens.append(tokens[0])\n",
    "        embedding_dict[i+2] = list(map(float, tokens[1:]))\n",
    "        i += 1\n",
    "        if i == MAX_VOCAB_SIZE:\n",
    "            break\n",
    "            \n",
    "    return embedding_dict, all_train_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n"
     ]
    }
   ],
   "source": [
    "_, val_data_x, val_data_y = get_string_tokenized_data(os.path.join(data_path, 'mnli_val.tsv'))\n",
    "_, train_data_x, train_data_y = get_string_tokenized_data(os.path.join(data_path, 'mnli_train.tsv'))\n",
    "\n",
    "fasttext_embedding_dict, all_fasttext_tokens = load_vectors('wiki-news-300d-1M.vec')\n",
    "\n",
    "token2id, id2token = build_vocab(all_fasttext_tokens)\n",
    "train_data_indices = token2index_dataset(train_data_x, token2id)\n",
    "val_data_indices = token2index_dataset(val_data_x, token2id)\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0;\n",
    "for x in train_data_indices:\n",
    "    if 1 in set(x[0]):\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35685"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/len(train_data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_path, 'mnli_val.tsv'), sep=\"\\t\")\n",
    "data = np.array(df);\n",
    "data = data.astype(str)\n",
    "val_genre_list = data[:, 3]\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_path, 'mnli_train.tsv'), sep=\"\\t\")\n",
    "data = np.array(df);\n",
    "data = data.astype(str)\n",
    "train_genre_list = data[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['travel', 'government', 'telephone', 'slate', 'fiction']\n"
     ]
    }
   ],
   "source": [
    "unique_genre = list(set(val_genre_list));\n",
    "print(unique_genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Pytorch Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_x, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_x = data_x;\n",
    "        self.target_list = target_list\n",
    "        \n",
    "        assert(len(data_x) == len(target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        prem_token_idx = self.data_x[key][0][:MAX_SENTENCE_LENGTH]\n",
    "        hyp_token_idx = self.data_x[key][1][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [prem_token_idx, hyp_token_idx, label]\n",
    "\n",
    "\n",
    "def encode_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    prem_data_list = []\n",
    "    hyp_data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    # print(\"collate batch: \", batch[0][0])\n",
    "    # batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        prem_padded_vec = np.pad(np.array(datum[0]),\n",
    "                                 pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[0]))),\n",
    "                                 mode=\"constant\", constant_values=0)\n",
    "        hyp_padded_vec = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        prem_data_list.append(prem_padded_vec)\n",
    "        hyp_data_list.append(hyp_padded_vec)\n",
    "    return [torch.from_numpy((np.array(prem_data_list))), torch.from_numpy(np.array(hyp_data_list)),\n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SNLIDataset(train_data_indices, train_data_y)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=encode_collate_func,\n",
    "#                                            shuffle=True)\n",
    "# val_dataset = SNLIDataset(val_data_indices, val_data_y)\n",
    "# val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "#                                            batch_size=BATCH_SIZE,\n",
    "#                                            collate_fn=encode_collate_func,\n",
    "#                                            shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(val_data_indices) == len(val_genre_list))\n",
    "assert(len(train_data_indices) == len(train_genre_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dict = {};\n",
    "train_loader_dict = {};\n",
    "\n",
    "for genre in unique_genre:\n",
    "    req_train_data_indices = np.array(train_data_indices)[ np.where(train_genre_list == genre)[0] ];\n",
    "    req_train_data_y =  np.array(train_data_y)[ np.where(train_genre_list == genre)[0] ];\n",
    "    \n",
    "    train_dataset_dict[genre] = SNLIDataset(req_train_data_indices, req_train_data_y)\n",
    "    train_loader_dict[genre] = torch.utils.data.DataLoader(dataset=train_dataset_dict[genre],\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_dict = {};\n",
    "val_loader_dict = {};\n",
    "\n",
    "for genre in unique_genre:\n",
    "    req_val_data_indices = np.array(val_data_indices)[ np.where(val_genre_list == genre)[0] ];\n",
    "    req_val_data_y =  np.array(val_data_y)[ np.where(val_genre_list == genre)[0] ];\n",
    "    \n",
    "    val_dataset_dict[genre] = SNLIDataset(req_val_data_indices, req_val_data_y)\n",
    "    val_loader_dict[genre] = torch.utils.data.DataLoader(dataset=val_dataset_dict[genre],\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=encode_collate_func,\n",
    "                                               shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic Functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for prem_data, hyp_data, labels in loader:\n",
    "        prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device),labels.to(device)\n",
    "        outputs = F.softmax(model(prem_data_batch, hyp_data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "def plot_acc(train_accs, val_accs, filename):\n",
    "    f = plt.figure()\n",
    "    plt.plot(train_accs, label='train');\n",
    "    plt.plot(val_accs, label='val');\n",
    "    plt.title(filename);\n",
    "    plt.legend()\n",
    "\n",
    "    f.savefig(os.path.join(filename[:3], filename + \".pdf\"), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, regularization, train_loader, val_loader):\n",
    "\n",
    "#     file_name = '_'.join([model_type, 'kernel_size='+str(kernel_size), 'hidden_size='+str(hidden_size), 'linear_hid_dim='+str(linear_hid_dim), \n",
    "#                        'combine_method='+str(combine_method), 'regularization='+str(regularization)]);\n",
    "#     print('\\n'.join([model_type, 'kernel_size='+str(kernel_size), 'hidden_size='+str(hidden_size), 'linear_hid_dim='+str(linear_hid_dim), \n",
    "#                        'combine_method='+str(combine_method), 'regularization='+str(regularization)]))\n",
    "#     sys.stdout.flush()\n",
    "\n",
    "\n",
    "    \n",
    "    learning_rate = 1e-3;\n",
    "    num_epochs = 2;\n",
    "    \n",
    "    dropout = (regularization == 'dropout')\n",
    "    \n",
    "\n",
    "#     if(model_type == 'cnn'):\n",
    "#         model = CNN(EMBED_SIZE , hidden_size, MAX_VOCAB_SIZE+2, kernel_size, linear_hid_dim, combine_method, dropout);\n",
    "#     elif(model_type == 'rnn'):\n",
    "#         model = RNN(EMBED_SIZE , hidden_size, MAX_VOCAB_SIZE+2,  linear_hid_dim, combine_method, dropout);\n",
    "#     else:\n",
    "#         error('invalid model type')\n",
    "        \n",
    "    model = model.to(device);\n",
    "    \n",
    "    print (\"When starting\")\n",
    "    print (\"Train Acc {}\".format(test_model(train_loader, model)))\n",
    "    print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    if regularization == 'weight_decay':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val = 0;\n",
    "    best_state_dict = None;\n",
    "    \n",
    "    train_acc_array = [];\n",
    "    val_acc_array = [];\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i, (prem, hyp, label) in enumerate(train_loader):\n",
    "            \n",
    "#             if i>300:\n",
    "#                 break;\n",
    "            model.train()\n",
    "            \n",
    "            prem_batch, hyp_batch, label_batch = prem.to(device), hyp.to(device), label.to(device);\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(prem_batch, hyp_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 300 iterations\n",
    "            if (i+1) % 300 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                \n",
    "                if val_acc > best_val:\n",
    "                    best_state_dict = model.state_dict();\n",
    "                    best_val = val_acc;\n",
    "                    \n",
    "                val_acc_array.append(val_acc);\n",
    "                train_acc = test_model(train_loader, model);\n",
    "                train_acc_array.append(train_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "#     plot_acc(train_acc_array, val_acc_array, file_name)\n",
    "    \n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Train Acc {}\".format(test_model(train_loader, model)))\n",
    "    print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "#     model.load_state_dict(best_state_dict)\n",
    "    \n",
    "#     return test_model(train_loader, model), test_model(val_loader, model), model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, kernel_size, linear_hidden_dim, combine_method, dropout):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        assert(kernel_size % 2 == 1);\n",
    "        assert(combine_method in ['concat', 'mul', 'add']);\n",
    "        \n",
    "        padding = int( (kernel_size-1)/2 );        \n",
    "\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.combine_method = combine_method;\n",
    "        self.dropout = dropout;\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.from_pretrained(torch.from_numpy(np.array(fasttext_embedding_dict)).cuda(), freeze = False)\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(np.array(fasttext_embedding_dict).copy()))\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "\n",
    "\n",
    "        \n",
    "        if combine_method == 'concat':\n",
    "            self.linear_layers1 = nn.Linear(hidden_size*2, linear_hidden_dim)\n",
    "        else:\n",
    "            self.linear_layers1 = nn.Linear(hidden_size, linear_hidden_dim)\n",
    "            \n",
    "        self.linear_layers2 =  nn.Linear(linear_hidden_dim, 3)\n",
    "        \n",
    "#         self.xavier_init(self.linear_layers1);\n",
    "#         self.xavier_init(self.linear_layers2);\n",
    "        \n",
    "        if self.dropout:\n",
    "            self.dropout_layer = nn.Dropout(0.5);\n",
    "            \n",
    "        \n",
    "    def xavier_init(self, layer):\n",
    "        torch.nn.init.xavier_normal_(layer.weight.data)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "    \n",
    "    def indivual_encoding(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.FloatTensor).to(device);\n",
    "       \n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        \n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        \n",
    "#         print(hidden.shape)\n",
    "        hidden = torch.max(hidden, 1)[0]\n",
    "#         print(hidden.shape)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, prem, hyp):\n",
    "        prem_vector = self.indivual_encoding(prem);\n",
    "        hyp_vector = self.indivual_encoding(hyp);\n",
    "        \n",
    "        if self.combine_method == 'concat':\n",
    "            final_code = torch.cat((prem_vector, hyp_vector), dim=1);\n",
    "        elif self.combine_method == 'mul':\n",
    "            final_code = prem_vector * hyp_vector;\n",
    "        elif self.combine_method == 'add':\n",
    "            final_code = prem_vector + hyp_vector;\n",
    "            \n",
    "\n",
    "        final_code = self.linear_layers1(final_code);\n",
    "        final_code = F.relu(final_code);\n",
    "        if self.dropout:\n",
    "            final_code = self.dropout_layer(final_code);\n",
    "        final_code = self.linear_layers2(final_code)\n",
    "            \n",
    "            \n",
    "        return final_code\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, linear_hidden_dim, combine_method, dropout):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.combine_method = combine_method;\n",
    "        self.dropout = dropout;\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(np.array(fasttext_embedding_dict).copy()))\n",
    "        \n",
    "        self.bi_gru = nn.GRU(emb_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        if combine_method == 'concat':\n",
    "            self.linear_layers1 = nn.Linear(hidden_size*2, linear_hidden_dim)\n",
    "        else:\n",
    "            self.linear_layers1 = nn.Linear(hidden_size, linear_hidden_dim)\n",
    "            \n",
    "        self.linear_layers2 =  nn.Linear(linear_hidden_dim, 3)\n",
    "        \n",
    "        if self.dropout:\n",
    "            self.dropout_layer = nn.Dropout(0.5);\n",
    "            \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.zeros(2, batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.FloatTensor).to(device)\n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        \n",
    "        # embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.cpu().numpy(), batch_first=True)\n",
    "        \n",
    "        output, hidden = self.bi_gru(embed, self.hidden)\n",
    "        hidden = torch.sum(hidden, dim = 0)\n",
    "        \n",
    "#         hidden = hidden.index_select(0, idx_unsort)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, prem, hyp):\n",
    "        batch_size, seq_len = prem.size()\n",
    "\n",
    "        prem_vector = self.encode(prem)\n",
    "        hyp_vector = self.encode(hyp)\n",
    "        \n",
    "        if self.combine_method == 'concat':\n",
    "            final_code = torch.cat((prem_vector, hyp_vector), dim=1);\n",
    "        elif self.combine_method == 'mul':\n",
    "            final_code = prem_vector * hyp_vector;\n",
    "        elif self.combine_method == 'add':\n",
    "            final_code = prem_vector + hyp_vector;\n",
    "            \n",
    "\n",
    "        final_code = self.linear_layers1(final_code);\n",
    "        final_code = F.relu(final_code);\n",
    "        if self.dropout:\n",
    "            final_code = self.dropout_layer(final_code);\n",
    "        final_code = self.linear_layers2(final_code)\n",
    "            \n",
    "            \n",
    "        return final_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Best Models - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kernel_size = 3;\n",
    "best_hidden_size = 100;\n",
    "best_combine_method = 'concat';\n",
    "best_regularization_cnn = 'dropout';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sm7582/.conda/envs/denoising/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type CNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "best_cnn = torch.load('./cnn_1/best_cnn.pth', map_location={'cuda:0': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travel Val Acc:  41.14052953156823\n",
      "government Val Acc:  41.732283464566926\n",
      "telephone Val Acc:  43.08457711442786\n",
      "slate Val Acc:  38.82235528942116\n",
      "fiction Val Acc:  38.391959798994975\n"
     ]
    }
   ],
   "source": [
    "for x in unique_genre:\n",
    "    val_acc = test_model(val_loader_dict[x], best_cnn)\n",
    "    print(x, 'Val Acc: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Best Models - RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regularization_rnn = 'none';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sm7582/.conda/envs/denoising/lib/python3.6/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "best_rnn = torch.load('./rnn/best_rnn.pth', map_location={'cuda:0': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travel Val Acc:  43.38085539714868\n",
      "government Val Acc:  46.55511811023622\n",
      "telephone Val Acc:  47.2636815920398\n",
      "slate Val Acc:  43.213572854291414\n",
      "fiction Val Acc:  50.35175879396985\n"
     ]
    }
   ],
   "source": [
    "for x in unique_genre:\n",
    "    val_acc = test_model(val_loader_dict[x], best_rnn)\n",
    "    print(x, 'Val Acc: ', val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Turning CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Genre:  travel\n",
      "When starting\n",
      "Train Acc 41.27979924717691\n",
      "Val Acc 41.14052953156823\n",
      "After training for 2 epochs\n",
      "Train Acc 60.70263488080301\n",
      "Val Acc 43.686354378818734\n",
      "\n",
      "Genre:  government\n",
      "When starting\n",
      "Train Acc 40.50991501416431\n",
      "Val Acc 41.732283464566926\n",
      "After training for 2 epochs\n",
      "Train Acc 61.653360803502444\n",
      "Val Acc 46.35826771653543\n",
      "\n",
      "Genre:  telephone\n",
      "When starting\n",
      "Train Acc 38.99297423887588\n",
      "Val Acc 43.08457711442786\n",
      "After training for 2 epochs\n",
      "Train Acc 60.37470725995316\n",
      "Val Acc 42.985074626865675\n",
      "\n",
      "Genre:  slate\n",
      "When starting\n",
      "Train Acc 40.188772975658225\n",
      "Val Acc 38.82235528942116\n",
      "After training for 2 epochs\n",
      "Train Acc 56.58221559860904\n",
      "Val Acc 38.02395209580838\n",
      "\n",
      "Genre:  fiction\n",
      "When starting\n",
      "Train Acc 41.0844629822732\n",
      "Val Acc 38.391959798994975\n",
      "After training for 2 epochs\n",
      "Train Acc 59.019812304483835\n",
      "Val Acc 39.49748743718593\n"
     ]
    }
   ],
   "source": [
    "for x in unique_genre:\n",
    "    print('\\nGenre: ', x);\n",
    "    best_cnn = torch.load('./cnn_1/best_cnn.pth', map_location={'cuda:0': 'cpu'})\n",
    "    train_and_evaluate(best_cnn, best_regularization_cnn,\n",
    "                                                  train_loader_dict[x], val_loader_dict[x]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Genre:  travel\n",
      "When starting\n",
      "Train Acc 46.148055207026346\n",
      "Val Acc 43.38085539714868\n",
      "After training for 2 epochs\n",
      "Train Acc 61.65621079046424\n",
      "Val Acc 52.342158859470466\n",
      "\n",
      "Genre:  government\n",
      "When starting\n",
      "Train Acc 48.31315992789081\n",
      "Val Acc 46.55511811023622\n",
      "After training for 2 epochs\n",
      "Train Acc 62.52897244398661\n",
      "Val Acc 54.82283464566929\n",
      "\n",
      "Genre:  telephone\n",
      "When starting\n",
      "Train Acc 46.51053864168618\n",
      "Val Acc 47.2636815920398\n",
      "After training for 2 epochs\n",
      "Train Acc 60.304449648711945\n",
      "Val Acc 52.63681592039801\n",
      "\n",
      "Genre:  slate\n",
      "When starting\n",
      "Train Acc 44.63487332339791\n",
      "Val Acc 43.213572854291414\n",
      "After training for 2 epochs\n",
      "Train Acc 58.991554893194234\n",
      "Val Acc 48.60279441117765\n",
      "\n",
      "Genre:  fiction\n",
      "When starting\n",
      "Train Acc 48.5140771637122\n",
      "Val Acc 50.35175879396985\n",
      "After training for 2 epochs\n",
      "Train Acc 62.877997914494266\n",
      "Val Acc 53.969849246231156\n"
     ]
    }
   ],
   "source": [
    "for x in unique_genre:\n",
    "    print('\\nGenre: ', x);\n",
    "    best_rnn = torch.load('./rnn/best_rnn.pth', map_location={'cuda:0': 'cpu'})\n",
    "    train_and_evaluate(best_rnn, best_regularization_rnn,\n",
    "                                                  train_loader_dict[x], val_loader_dict[x]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
