{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './hw2_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets improve libraries that we are going to be used in this lab session\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl\n",
    "import random\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "random.seed(134)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 150000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 50\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_label_map = {'neutral':0,\n",
    "#                 'contradiction': 1, \n",
    "#                 'entailment': 2};\n",
    "\n",
    "y_label_map = {'contradiction':0,'neutral':1,'entailment':2}\n",
    "\n",
    "def get_string_tokenized_data(data_mat_path):\n",
    "    df = pd.read_csv(os.path.join(data_mat_path), sep=\"\\t\")\n",
    "    data = np.array(df);\n",
    "    data = data.astype(str)\n",
    "    \n",
    "    tokenized_data_x = len(data) * [None];\n",
    "    y_labels = [y_label_map[x] for x in data[:, 2] ];\n",
    "    \n",
    "    all_tokens = [];\n",
    "    \n",
    "    for i,x in enumerate(data):\n",
    "        tokenized_data_x[i] = [x[0].split(), x[1].split()];\n",
    "        all_tokens += (tokenized_data_x[i][0] + tokenized_data_x[i][1])\n",
    "\n",
    "    return all_tokens, tokenized_data_x, y_labels\n",
    "        \n",
    "\n",
    "\n",
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data, token2id):\n",
    "    indices_data = []\n",
    "    for tokens1, tokens2 in tokens_data:\n",
    "        index_list1 = [token2id[token] if token in token2id else UNK_IDX for token in tokens1]\n",
    "        index_list2 = [token2id[token] if token in token2id else UNK_IDX for token in tokens2]\n",
    "        indices_data.append([index_list1, index_list2])\n",
    "    return indices_data\n",
    "\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    embedding_dict = np.random.randn(MAX_VOCAB_SIZE+2, EMBED_SIZE)\n",
    "    all_train_tokens = []\n",
    "    i = 0\n",
    "    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        all_train_tokens.append(tokens[0])\n",
    "        embedding_dict[i+2] = list(map(float, tokens[1:]))\n",
    "        i += 1\n",
    "        if i == MAX_VOCAB_SIZE:\n",
    "            break\n",
    "            \n",
    "    return embedding_dict, all_train_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "_, val_data_x, val_data_y = get_string_tokenized_data(os.path.join(data_path, 'snli_val.tsv'))\n",
    "_, train_data_x, train_data_y = get_string_tokenized_data(os.path.join(data_path, 'snli_train.tsv'))\n",
    "\n",
    "fasttext_embedding_dict, all_fasttext_tokens = load_vectors('wiki-news-300d-1M.vec')\n",
    "\n",
    "token2id, id2token = build_vocab(all_fasttext_tokens)\n",
    "train_data_indices = token2index_dataset(train_data_x, token2id)\n",
    "val_data_indices = token2index_dataset(val_data_x, token2id)\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0;\n",
    "for x in train_data_indices:\n",
    "    if 1 in set(x[0]):\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0712"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count/len(train_data_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Pytorch Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_x, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens\n",
    "        @param target_list: list of newsgroup targets\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_x = data_x;\n",
    "        self.target_list = target_list\n",
    "        \n",
    "        assert(len(data_x) == len(target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        prem_token_idx = self.data_x[key][0][:MAX_SENTENCE_LENGTH]\n",
    "        hyp_token_idx = self.data_x[key][1][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [prem_token_idx, hyp_token_idx, label]\n",
    "\n",
    "\n",
    "def encode_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    prem_data_list = []\n",
    "    hyp_data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    # print(\"collate batch: \", batch[0][0])\n",
    "    # batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        prem_padded_vec = np.pad(np.array(datum[0]),\n",
    "                                 pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[0]))),\n",
    "                                 mode=\"constant\", constant_values=0)\n",
    "        hyp_padded_vec = np.pad(np.array(datum[1]),\n",
    "                                pad_width=((0, MAX_SENTENCE_LENGTH - len(datum[1]))),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        prem_data_list.append(prem_padded_vec)\n",
    "        hyp_data_list.append(hyp_padded_vec)\n",
    "    return [torch.from_numpy((np.array(prem_data_list))), torch.from_numpy(np.array(hyp_data_list)),\n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SNLIDataset(train_data_indices, train_data_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = SNLIDataset(val_data_indices, val_data_y)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=encode_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic Functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for prem_data, hyp_data, labels in loader:\n",
    "        prem_data_batch, hyp_data_batch, label_batch = prem_data.to(device), hyp_data.to(device),labels.to(device)\n",
    "        outputs = F.softmax(model(prem_data_batch, hyp_data_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "def plot_acc(train_accs, val_accs, filename):\n",
    "    f = plt.figure()\n",
    "    plt.plot(train_accs, label='train');\n",
    "    plt.plot(val_accs, label='val');\n",
    "    plt.title(filename);\n",
    "    plt.legend()\n",
    "\n",
    "    f.savefig(os.path.join(filename[:3], filename + \".pdf\"), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model_type = 'cnn', kernel_size = 3, hidden_size = 3, linear_hid_dim = 10, \n",
    "                       combine_method = 'concat', regularization = 'none'):\n",
    "\n",
    "    file_name = '_'.join([model_type, 'kernel_size='+str(kernel_size), 'hidden_size='+str(hidden_size), 'linear_hid_dim='+str(linear_hid_dim), \n",
    "                       'combine_method='+str(combine_method), 'regularization='+str(regularization)]);\n",
    "    print('\\n'.join([model_type, 'kernel_size='+str(kernel_size), 'hidden_size='+str(hidden_size), 'linear_hid_dim='+str(linear_hid_dim), \n",
    "                       'combine_method='+str(combine_method), 'regularization='+str(regularization)]))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    learning_rate = 1e-3;\n",
    "    num_epochs = 10;\n",
    "    \n",
    "    dropout = (regularization == 'dropout')\n",
    "    \n",
    "\n",
    "    if(model_type == 'cnn'):\n",
    "        model = CNN(EMBED_SIZE , hidden_size, MAX_VOCAB_SIZE+2, kernel_size, linear_hid_dim, combine_method, dropout);\n",
    "    elif(model_type == 'rnn'):\n",
    "        model = RNN(EMBED_SIZE , hidden_size, MAX_VOCAB_SIZE+2,  linear_hid_dim, combine_method, dropout);\n",
    "    else:\n",
    "        error('invalid model type')\n",
    "        \n",
    "    model = model.to(device);\n",
    "\n",
    "    # Criterion and Optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    if regularization == 'weight_decay':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val = 0;\n",
    "    best_state_dict = None;\n",
    "    \n",
    "    train_acc_array = [];\n",
    "    val_acc_array = [];\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for i, (prem, hyp, label) in enumerate(train_loader):\n",
    "            \n",
    "#             if i>300:\n",
    "#                 break;\n",
    "            model.train()\n",
    "            \n",
    "            prem_batch, hyp_batch, label_batch = prem.to(device), hyp.to(device), label.to(device);\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(prem_batch, hyp_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 300 iterations\n",
    "            if (i+1) % 300 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                \n",
    "                if val_acc > best_val:\n",
    "                    best_state_dict = model.state_dict();\n",
    "                    best_val = val_acc;\n",
    "                    \n",
    "                val_acc_array.append(val_acc);\n",
    "                train_acc = test_model(train_loader, model);\n",
    "                train_acc_array.append(train_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Train Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc, train_acc))\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "    plot_acc(train_acc_array, val_acc_array, file_name)\n",
    "    \n",
    "    print (\"After training for {} epochs\".format(num_epochs))\n",
    "    print (\"Train Acc {}\".format(test_model(train_loader, model)))\n",
    "    print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    model.load_state_dict(best_state_dict)\n",
    "    \n",
    "    return test_model(train_loader, model), test_model(val_loader, model), model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, kernel_size, linear_hidden_dim, combine_method, dropout):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        assert(kernel_size % 2 == 1);\n",
    "        assert(combine_method in ['concat', 'mul', 'add']);\n",
    "        \n",
    "        padding = int( (kernel_size-1)/2 );        \n",
    "\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.combine_method = combine_method;\n",
    "        self.dropout = dropout;\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.from_pretrained(torch.from_numpy(np.array(fasttext_embedding_dict)).to(device), freeze = False)\n",
    "#         self.embedding.weight.data.copy_(torch.from_numpy(np.array(fasttext_embedding_dict).copy()))\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=padding)\n",
    "\n",
    "\n",
    "        \n",
    "        if combine_method == 'concat':\n",
    "            self.linear_layers1 = nn.Linear(hidden_size*2, linear_hidden_dim)\n",
    "        else:\n",
    "            self.linear_layers1 = nn.Linear(hidden_size, linear_hidden_dim)\n",
    "            \n",
    "        self.linear_layers2 =  nn.Linear(linear_hidden_dim, 3)\n",
    "        \n",
    "#         self.xavier_init(self.linear_layers1);\n",
    "#         self.xavier_init(self.linear_layers2);\n",
    "        \n",
    "        if self.dropout:\n",
    "            self.dropout_layer = nn.Dropout(0.5);\n",
    "            \n",
    "        \n",
    "    def xavier_init(self, layer):\n",
    "        torch.nn.init.xavier_normal_(layer.weight.data)\n",
    "        layer.bias.data.fill_(0.01)\n",
    "    \n",
    "    def indivual_encoding(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.FloatTensor).to(device);\n",
    "       \n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        \n",
    "        hidden = self.conv1(embed.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "\n",
    "        hidden = self.conv2(hidden.transpose(1,2)).transpose(1,2)\n",
    "        hidden = F.relu(hidden.contiguous().view(-1, hidden.size(-1))).view(batch_size, seq_len, hidden.size(-1))\n",
    "        \n",
    "#         print(hidden.shape)\n",
    "        hidden = torch.max(hidden, 1)[0]\n",
    "#         print(hidden.shape)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self, prem, hyp):\n",
    "        prem_vector = self.indivual_encoding(prem);\n",
    "        hyp_vector = self.indivual_encoding(hyp);\n",
    "        \n",
    "        if self.combine_method == 'concat':\n",
    "            final_code = torch.cat((prem_vector, hyp_vector), dim=1);\n",
    "        elif self.combine_method == 'mul':\n",
    "            final_code = prem_vector * hyp_vector;\n",
    "        elif self.combine_method == 'add':\n",
    "            final_code = prem_vector + hyp_vector;\n",
    "            \n",
    "\n",
    "        final_code = self.linear_layers1(final_code);\n",
    "        final_code = F.relu(final_code);\n",
    "        if self.dropout:\n",
    "            final_code = self.dropout_layer(final_code);\n",
    "        final_code = self.linear_layers2(final_code)\n",
    "            \n",
    "            \n",
    "        return final_code\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, vocab_size, linear_hidden_dim, combine_method, dropout):\n",
    "\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.combine_method = combine_method;\n",
    "        self.dropout = dropout;\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD_IDX)\n",
    "        self.embedding.from_pretrained(torch.from_numpy(np.array(fasttext_embedding_dict)).to(device), freeze = False)\n",
    "        \n",
    "        self.bi_gru = nn.GRU(emb_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        if combine_method == 'concat':\n",
    "            self.linear_layers1 = nn.Linear(hidden_size*2, linear_hidden_dim)\n",
    "        else:\n",
    "            self.linear_layers1 = nn.Linear(hidden_size, linear_hidden_dim)\n",
    "            \n",
    "        self.linear_layers2 =  nn.Linear(linear_hidden_dim, 3)\n",
    "        \n",
    "        if self.dropout:\n",
    "            self.dropout_layer = nn.Dropout(0.5);\n",
    "            \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.zeros(2, batch_size, self.hidden_size).to(device)\n",
    "        return hidden\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \n",
    "        batch_size, seq_len = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embed = self.embedding(x)\n",
    "        m = (x == 1)\n",
    "        m = m.unsqueeze(2).repeat(1, 1, EMBED_SIZE).type(torch.FloatTensor).to(device)\n",
    "        embed = m * embed + (1-m) * embed.clone().detach()\n",
    "        \n",
    "        # embed = torch.nn.utils.rnn.pack_padded_sequence(embed, lengths.cpu().numpy(), batch_first=True)\n",
    "        \n",
    "        output, hidden = self.bi_gru(embed, self.hidden)\n",
    "        hidden = torch.sum(hidden, dim = 0)\n",
    "        \n",
    "#         hidden = hidden.index_select(0, idx_unsort)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, prem, hyp):\n",
    "        batch_size, seq_len = prem.size()\n",
    "\n",
    "        prem_vector = self.encode(prem)\n",
    "        hyp_vector = self.encode(hyp)\n",
    "        \n",
    "        if self.combine_method == 'concat':\n",
    "            final_code = torch.cat((prem_vector, hyp_vector), dim=1);\n",
    "        elif self.combine_method == 'mul':\n",
    "            final_code = prem_vector * hyp_vector;\n",
    "        elif self.combine_method == 'add':\n",
    "            final_code = prem_vector + hyp_vector;\n",
    "            \n",
    "\n",
    "        final_code = self.linear_layers1(final_code);\n",
    "        final_code = F.relu(final_code);\n",
    "        if self.dropout:\n",
    "            final_code = self.dropout_layer(final_code);\n",
    "        final_code = self.linear_layers2(final_code)\n",
    "            \n",
    "            \n",
    "        return final_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuninig and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type_list = ['cnn', 'rnn']\n",
    "hidden_size_list = [50, 100, 300, 500]\n",
    "kernel_size_list = [3, 5]\n",
    "linear_hid_dim_list = [400]\n",
    "regularization_list = ['none', 'dropout', 'weight_decay'];\n",
    "# regularization_list = ['none']\n",
    "combine_method_list = ['concat', 'mul', 'add']\n",
    "# combine_method_list = ['concat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(model_type_list) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_param(model):\n",
    "     return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 45094803\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 45129803\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 45074803\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 45109803\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 45074803\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 45109803\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 45202403\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 45282403\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 45162403\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 45242403\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 45162403\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 45242403\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 45782803\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 46142803\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 45662803\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 46022803\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 45662803\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 46022803\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 46603203\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 47403203\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 46403203\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 47203203\n",
      "cnn\n",
      "kernel_size=3\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 46403203\n",
      "cnn\n",
      "kernel_size=5\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 47203203\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 45147803\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 45127803\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=50\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 45127803\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 45323403\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 45283403\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=100\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 45283403\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 46325803\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 46205803\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=300\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 46205803\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=concat\n",
      "params= 47808203\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=mul\n",
      "params= 47608203\n",
      "rnn\n",
      "kernel_size=None\n",
      "hidden_size=500\n",
      "linear_hid_dim=400\n",
      "combine_method=add\n",
      "params= 47608203\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('parameters.txt'), 'w') as thefile:\n",
    "    for model_type in model_type_list:\n",
    "        for hidden_size in hidden_size_list:\n",
    "            for linear_hid_dim in linear_hid_dim_list:\n",
    "#                 for regularization in regularization_list:\n",
    "                    for combine_method in combine_method_list:\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                                if(model_type == 'cnn'):\n",
    "                                    for kernel_size in kernel_size_list:\n",
    "                                    \n",
    "                                        model = CNN(EMBED_SIZE , hidden_size, MAX_VOCAB_SIZE+2, kernel_size, linear_hid_dim, combine_method, False);\n",
    "                                        model = model.to(device);\n",
    "                                        params = count_param(model)\n",
    "                                        \n",
    "                                        thefile.write(','.join([model_type, str(kernel_size), str(hidden_size), str(linear_hid_dim), \n",
    "                                                           str(combine_method), \n",
    "                                                               str(params)])+'\\n')\n",
    "                                        \n",
    "                                        print('\\n'.join([model_type, 'kernel_size='+str(kernel_size), 'hidden_size='+str(hidden_size), 'linear_hid_dim='+str(linear_hid_dim), \n",
    "                                           'combine_method='+str(combine_method)]))\n",
    "\n",
    "                                        print('params=', params);\n",
    "                                else:\n",
    "                                        kernel_size = None;\n",
    "                                        \n",
    "                                        model = RNN(EMBED_SIZE , hidden_size, MAX_VOCAB_SIZE+2,  linear_hid_dim, combine_method, False);\n",
    "                                        model = model.to(device);\n",
    "                                        params = count_param(model)\n",
    "                                        \n",
    "                                        thefile.write(','.join([model_type,  str(hidden_size), str(linear_hid_dim), \n",
    "                                                           str(combine_method), \n",
    "                                                               str(params)])+'\\n')\n",
    "                                        print('\\n'.join([model_type, 'kernel_size='+str(kernel_size), 'hidden_size='+str(hidden_size), 'linear_hid_dim='+str(linear_hid_dim), \n",
    "                                           'combine_method='+str(combine_method)]))\n",
    "\n",
    "                                        print('params=', params);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_type_list)\n",
    "print('\\n'.join(['best_val_acc: '+ str(best_acc), \n",
    "                    'best_hidden_size: '+str(best_hidden_size), 'best_linear_hid_dim: '+str(best_linear_hid_dim),\n",
    "                    'best_kernel_size: '+str(best_kernel_size), 'best_combine_method: '+combine_method]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
